---
author: Arthur Katossky & Rémi Pépin
output: html_document
---

# Large-scale machine-learning — Introduction

## Motivation

## Goal

## Outline & schedule

Course is common with 2A-3A.

1h30+1h30 Introduction to cloud computing

3 x 3h Tutorial.

Outline:

**Courses:**
- **Course 0:** Introduction to cloud computing (1h30)
- **Course 1:** Big Data in a nutshell (3h, January 13)
- **Course 2:** Computing parallelization principles (3h, January 17)
- **Course 3:** Parallelized computing on distributed data (2 × 1h30, January 28, February 6)

**Tutorials:**
- **Tutorial 0:** Hands on AWS (1h30)
- **Tutorial 1:** How to optimize a statistical algorithm? (3h, January, 20)
- **Tutorial 2:** How to speed up the computation of a statistical summary? (3h, February, 6)
- **Tutorial 3:** How to accelerate a neural network? (3h, February, 7)

## Evaluation

## Material

All material will be available on Moodle.

Recommended books:
...
...

# Introduction

<!-- COURS 1

What is big data? what is it not?
The V's of big data. What are we concerned with?
What big data is **not**?

Computer science: survival kit
> processor, ram, disk
> CPU, GPU...
> (storing numbers e.g. floating point calculation)

Problems:
- too big to fit in RAM
- too big to fit on disk
- computation is too slow
- data arrive continuously and too fast
- too costly!

How to fix them?
- going lower level (= more task-specific computation)
- algorithmic optimisation
- sampling
- parellizing computation
- distributing data

Costs:
- storing less (sampling!)
- computing less (sampling!)
- going from RAM to disk
- using smaller computing units

Only the last 2 are usually labeled "big data" even though they should last-resot options.

2-3 exemples

-->
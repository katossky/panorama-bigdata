---
title: "Large-scale machine-learning"
author: "Arthur Katossky & Rémi Pépin"
date: "January 2020"
subtitle: Tutorial 1 — How to optimize a statistical algorithm?
institute: ENSAI (Rennes, France)
---

<!--
TO DO:
[ ] add title
[ ] add sample section
[ ] merge the two pages into one
[ ] change "sliding mean" to "moving average"
[ ] test tutorial
[ ] add outline
[ ] clean up Remi's code
[ ] longer sample section
-->

# TP1 - Code profiling, parallelization, big data  and cloud computing architecture?

You will find the code of this practical session on moodle.

## 1- Run the code

1. Run the code. 
2. Does it take time ? Any idea why ?
3. Find a way to make it run quickly

## 2- Code profiling

Now the code is running, we will try to find how many time take each part and prioritize our work

1. Add instruction to the code to calculate the time took by each part. This [page](https://www.r-bloggers.com/5-ways-to-measure-running-time-of-r-code/) can help you
2. What are the empirical time complexity of each part ? You can make a time-input size plot to help you.
3. What are the parts to optimize in priority ?

 ## 3- Optimization

1. SQL optimization

   1. Do we really need to do

   ```SQL
   	SELECT * FROM flight
   ```

   ?

   2. Can we transfer some treatments done in R to the DB ? If so, update the R code

2. Optimize the computation of the mean of flight number by month.

   1. With R statements
   2. With SQL statements
   3. Which is the faster ?

3. Think about how sliding mean works. How can you change the code to optimize it ?
4. Do the kernel regressions depend of each other ? Propose a way to accelerate them.

5. We do not need to perform the actual complete computation if we are ready to accept some imprecision. But there is a trade-off between computation-time and precision.

5.1 Let's assume that a sample is taken from a (potentially infinite) population with a (known or knowable) data-generating process. An approach to the measure of uncertainty is **asymptotical inference**.
    a.  — In this context, recall the asymptotical distribution of the mean estimator of a random sample of size $n$, as n tends to infinity. How can you estimate the distribution? <!-- you need an estimation of the variance -->
    b. Modify the database request, so that it returns a _random_ sample of size $n$. You may try first with $n=40$ then wrap your code in a function for arbitrary $n$.
       <!-- ORDER BY random() LIMIT 40 -->
    c. On the same graph, draw violin plots of the estimated distribution for sub-samples of size $40 \times 2^k$ for $k=1,...,10$ from the downloaded sample. You may want to complete the following code, assuming your sample is named `fligths`.
    
```{r, eval=FALSE}
library(dplyr)
library(tictoc)

x         <- mean(flights$passengers) + seq(-200, 200, by=10)
probs     <- tibble() # an empty data frame with dplyr package
estimates <- tibble()

for(k in 1:10){

  n <- 40*2^k
  
  tic()
  flights <- ......
  m <- mean(flights$passengers[1:n])
  s <-   sd(flights$passengers[1:n])
  t <- toc()
  
  
  estimates[k, "n"] <- n
  estimates[k, "m"] <- m
  estimates[k, "s"] <- s
  estimates[k, "t"] <- t
  
  probs[k,"x"] <- x
  probs[k,"f"] <- dnorm(x,m,s)
  probs[k,"k"] <- k
  
     probs,
     tibble(x = x, f = dnorm(x,m,s), k = k)
  )
}

probs %>% ggplot(aes(x=x, y=f, group=k))+geom_violin()
```

d. Make a plot of $\hat{\sigma}$ against computation time. What would be an acceptable level of precision here?


<!--

**mean estimator:** mean over the sample
**law of the mean estimator:** converges to 

-->


5.2 **Mean of a random variable** — Let's assume that a sample is taken from a (potentially infinite) population with a knowable data-generating process. In this context, recall the asymptotical distribution of the mean estimator.

5.3 **Mean f boostrap** — Let's assume that a sample is taken from a (potentially infinite) population with a knowable data-generating process. In this context, recall the asymptotical distribution of the mean estimator.

5.4 **Mean of a random variable** — Let's assume that a sample is taken from a (potentially infinite) population with a knowable data-generating process. In this context, recall the asymptotical distribution of the mean estimator.




Statistics ? (sampling, confidence interval)

5.1 



Recall the law of the mean on a sample of time n.



## 4- On a bigger machine ?

Create a powerful EC2 instance to run you code (like a m5.2xlarge with 8 cores and 32 GB or Ram) to run your code and see if there is a big difference. To do this you have to

1. Create an EC2 instance with ubuntu on it

2. Authorize SSH connection to the instance (see : "Etablir une connexion SSH avec votre cluster" in the previous session)

3. Connect with SSH with a SSH tunnel from port 8157
4. Install and configure foxyproxy (see : "Installer FoxyProxy" in the previous session. It seems that foxyproxy doesn't work well with chrome, please use Firefox)
5. Install Rstudio server

```bash
# Install R
sudo apt-get install r-base
# To install local package
sudo apt-get install gdebi-core
# Donwload Rstudio server
wget https://download2.rstudio.org/server/trusty/amd64/rstudio-server-1.2.5033-amd64.deb
# Install it
sudo gdebi rstudio-server-1.2.5033-amd64.deb
```

6. Create a Rstudio user

```bash
# Make User
sudo useradd -m rstudio-user
sudo passwd rstudio-user
```



6. Connect to Rstudio server : https://[public-DNS]:8787 with [public-DNS] the public DNS of the instance

You will find all the steps and explanation in the previous practical session. The main differences are :

- you don't create an EMR cluster, just an EC2 instance
- you have to connect to the EC2 instance with it DNS public address

## 5- On a spark cluster ?

Create a spark cluster, install Rstudio-server on it (previous session) and update the code to use spark function. Run the code.

- sparklyr documentation : https://spark.rstudio.com/